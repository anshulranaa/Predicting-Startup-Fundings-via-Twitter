{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Constants\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 10000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data():\n",
    "    (training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=MAX_WORDS)\n",
    "    data = np.concatenate((training_data, testing_data), axis=0)\n",
    "    targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "    return data, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "data = vectorize(data)\n",
    "targets = np.array(targets).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences):\n",
    "    results = np.zeros((len(sequences), MAX_WORDS))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, activation='relu', input_shape=(MAX_WORDS,), kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_train_model(model, train_x, train_y, test_x, test_y):\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    results = model.fit(train_x, train_y, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                        validation_data=(test_x, test_y), callbacks=[early_stopping])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(text, word_index):\n",
    "    tokens = text.lower().split()\n",
    "    indices = [word_index.get(token, 0) for token in tokens if word_index.get(token, 0) < MAX_WORDS]\n",
    "    return indices[:MAX_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "    def extract_hashtags(text):\n",
    "        return re.findall(r\"#(\\w+)\", text)\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "        text = re.sub(r\"@\\w+\", \"\", text)\n",
    "        text = re.sub(r\"#\\w+\", \"\", text)\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        tokens = text.split()\n",
    "        tokens = [word for word in tokens if word not in stop_words_set]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    df['hashtags'] = df['tweet'].apply(extract_hashtags)\n",
    "    df['processed_tweet'] = df['tweet'].apply(preprocess_text)\n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(file_path, model, word_index):\n",
    "    df = pd.read_csv(file_path)\n",
    "    senti_scores = []\n",
    "\n",
    "    for i in df[\"processed_tweet\"]:\n",
    "        input_indices = text_to_index(i, word_index)\n",
    "        if input_indices:\n",
    "            senti_scores.append(np.mean(model.predict(vectorize_sequences([input_indices]))))\n",
    "        else:\n",
    "            senti_scores.append(0.5)\n",
    "\n",
    "    df[\"senti_score\"] = senti_scores\n",
    "    df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data, targets = load_imdb_data()\n",
    "    data = vectorize_sequences(data)\n",
    "    targets = np.array(targets).astype(\"float32\")\n",
    "    test_x = data[:10000]\n",
    "    test_y = targets[:10000]\n",
    "    train_x = data[40000:]\n",
    "    train_y = targets[40000:]\n",
    "\n",
    "    model = build_model()\n",
    "    results = compile_and_train_model(model, train_x, train_y, test_x, test_y)\n",
    "\n",
    "    scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
    "\n",
    "    folder_path = \"/Users/anshulrana/Desktop/vsc/python/Predicting_Success_Of_Startups/twitter_Data/tweets_data\"\n",
    "    for i in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, i)\n",
    "        if \".csv\" in file_path:\n",
    "            preprocess_tweets(file_path)\n",
    "            sentiment_analysis(file_path, model, index)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

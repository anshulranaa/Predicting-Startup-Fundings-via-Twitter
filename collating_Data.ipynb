{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter Data\n",
    "twitter_folder = \"/Users/anshulrana/Desktop/vsc/python/Predicting_Success_Of_Startups/twitter_Data/tweets_data\"\n",
    "\n",
    "# Collating File Paths\n",
    "twitter_file_paths = []\n",
    "for i in os.listdir(twitter_folder):\n",
    "    file_path = os.path.join(twitter_folder, i)\n",
    "    if \".csv\" in file_path:\n",
    "        twitter_file_paths.append(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funding Data\n",
    "funding_folder = \"/Users/anshulrana/Desktop/vsc/python/Predicting_Success_Of_Startups/funding_Data\"\n",
    "#Funding File Path \n",
    "funding_file = os.path.join(funding_folder, \"startup_funding_cleaned.csv\")\n",
    "\n",
    "funding_df = pd.read_csv(funding_file)\n",
    "funding_df['Company'] = funding_df['Company'].str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function to combine datasets\n",
    "def combine_datasets(twitter_data_path, company_name, combined_data):\n",
    "    twitter_data = pd.read_csv(twitter_data_path)\n",
    "\n",
    "    # Twitter Data Analysis\n",
    "    if 'hashtags' in twitter_data.columns:\n",
    "        twitter_data['hashtags'] = twitter_data['hashtags'].apply(ast.literal_eval)\n",
    "        avg_hashtags = twitter_data['hashtags'].apply(len).mean()\n",
    "    else:\n",
    "        avg_hashtags = 0\n",
    "\n",
    "    def count_mentions(tweet):\n",
    "        mentions = re.findall(r'@(\\w+)', tweet)\n",
    "        return len(mentions)\n",
    "\n",
    "    def count_links(tweet):\n",
    "        links = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet)\n",
    "        return len(links)\n",
    "\n",
    "    replies = twitter_data['replies'].mean()\n",
    "    avg_mentions = twitter_data['tweet'].apply(count_mentions).mean()\n",
    "    avg_tweet_length = twitter_data['tweet'].apply(len).mean()\n",
    "    avg_likes = twitter_data['likes'].mean() if 'likes' in twitter_data.columns else 0\n",
    "    avg_retweets = twitter_data['retweets'].mean() if 'retweets' in twitter_data.columns else 0\n",
    "    avg_senti_score = twitter_data['senti_score'].mean() if 'senti_score' in twitter_data.columns else 0\n",
    "    avg_links_mentioned = twitter_data['tweet'].apply(count_links).mean()\n",
    "\n",
    "    # Twitter metrics DataFrame\n",
    "    twitter_metrics = pd.DataFrame({\n",
    "        \"Company\": company_name.lower(),\n",
    "        \"avg_likes\": [avg_likes],\n",
    "        \"avg_retweets\": [avg_retweets],\n",
    "        \"avg_hashtags\": [avg_hashtags],\n",
    "        \"avg_senti\": [avg_senti_score],\n",
    "        \"avg_mentions\": [avg_mentions],\n",
    "        \"avg_tweet_length\": [avg_tweet_length],\n",
    "        \"avg_links_mentioned\": [avg_links_mentioned],\n",
    "        \"avg_replies\": [replies]\n",
    "    })\n",
    "\n",
    "\n",
    "    # \n",
    "    combined_data = pd.concat([combined_data, twitter_metrics], ignore_index=True)\n",
    "    return combined_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame for combined data\n",
    "combined_all_data = pd.DataFrame()\n",
    "\n",
    "# Paths to the data files\n",
    "funding_data_path = \"/Users/anshulrana/Desktop/vsc/python/Predicting_Success_Of_Startups/funding_Data/startup_funding_cleaned.csv\" # Update this with the correct path\n",
    "twitter_folder = \"/Users/anshulrana/Desktop/vsc/python/Predicting_Success_Of_Startups/twitter_Data/tweets_data\"   # Update this with the correct path\n",
    "\n",
    "for i in os.listdir(twitter_folder):\n",
    "    file_path = os.path.join(twitter_folder, i)\n",
    "    if \".csv\" in file_path:\n",
    "        company_name = i.split(\".\")[0]\n",
    "        combined_all_data = combine_datasets(file_path, company_name, combined_all_data)\n",
    "        combined_all_data['Company'] = combined_all_data['Company'].str.lower()\n",
    "\n",
    "# Forward fill the 'Company' column to associate the values correctly\n",
    "combined_all_data['Company'] = combined_all_data['Company'].ffill()\n",
    "# Now we will drop duplicate rows for each company, keeping the first occurrence\n",
    "cleaned_data = combined_all_data.drop_duplicates(subset=['Company'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.to_csv(\"combined_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(cleaned_data, funding_df, on='Company', how='left')\n",
    "combined_df.to_csv(\"final_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the repeating companies, retain the one with the smaller value of year\n",
    "combined_df['Year'] = combined_df['Year'].astype(int)\n",
    "combined_df = combined_df.sort_values(by=['Year'], ascending=True)\n",
    "combined_df = combined_df.drop_duplicates(subset=['Company'], keep='first')\n",
    "combined_df.to_csv(\"naya_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Including market analysis data\n",
    "market_folder = \"/Users/anshulrana/Desktop/vsc/python/Predicting_Success_Of_Startups/market_analysis\"\n",
    "gdp = pd.read_csv(os.path.join(market_folder, \"gdp_data.csv\"))\n",
    "\n",
    "#Add another column of GDP that adds the GDP growth rate for the value of the Year\n",
    "combined_df['Growth'] = combined_df['Year'].apply(lambda x: gdp[gdp['Year'] == x]['Growth'].values[0])\n",
    "\n",
    "combined_df.to_csv('bleh.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include fdi data\n",
    "fdi = pd.read_csv(os.path.join(market_folder, \"india_fdi_data.csv\"))\n",
    "combined_df['FDI'] = combined_df['Year'].apply(lambda x: fdi[fdi['Year'] == x]['FDI Net Inflows (% of GDP)'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('bleh.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
